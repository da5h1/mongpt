# mongpt
Инструкционная языковая модель для монгольского языка.

В рамках данного проекта монгольская языковая модель была обучена на наборе переведенных инструкций. В качестве базовой модели использовалсь mGPT-1.3B-mongol. В качестве датасета с инструкциями я использовал databricks-dolly-15k. Этот датасет был переведен на монгольский язык с помощью трех моделей-автопереводчиков. Я отбирал те переводы, которые достаточно похожи друг на друга. 

Для оптимизации расхода памяти я использовал mixed-presicion. Модель обучалась в течении 15 эпох на видеокарте NVIDIA A100, размер батча - 4, количество батчей - 1882. Тренировочный датасет содержит 7527 примеров, валидационный - 396. В качестве метрики обучения был использован BERTScore, в качестве модели BERT - мультиязычная xlm-roberta-large, эмбеддинги были взяты с 17-го слоя. 

Ниже вы можете видеть метрику обучения каждую эпоху.

![image](https://github.com/user-attachments/assets/046a7420-d02e-4607-ab57-d20ab483ca5e)

Так же я провел подбор параметров генерации на валидационном датасете. Я случайно подбирал параметры в некоторых дискретных границах, пока мне не удалось максимизировать значение метрики. Ниже вы можете видеть оптимальную конфигурацию параметров.

*min new tokens - 2*
*no repeat ngram size - 0*
*do sample - False*
*top k - 5*
*top p - 0.5*

Так же были проведены эксперименты с обучением с подводкой. Я явно указал модели следующую инструкцию на монгольском языке: "Напиши ответ для задания. Используй грамотный монгольский язык. Представь, что ты монгольский писатель Сэнгийн Эрдэнэ. Используй кириллицу."

Такой метод не привел к существенному увеличению метрики.

На тестовой выборке модель выдала значение BERTScore - *значение*.

Ниже вы можете видеть примеры самых удачных ответов модели и их перевод, полученный с помощью автопереводчика.

*лучшие результаты*

